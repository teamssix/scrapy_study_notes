2019-12-24 11:12:04 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:12:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:12:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:12:04 [scrapy.extensions.telnet] INFO: Telnet Password: 5aea82f87b8521ed
2019-12-24 11:12:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:12:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:12:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:12:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:12:04 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:12:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:12:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:12:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:12:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:12:05 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:12:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6508,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.029192,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 12, 5, 748857),
 'log_count/DEBUG': 2,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 12, 4, 719665)}
2019-12-24 11:12:05 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 11:17:35 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:17:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:17:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:17:35 [scrapy.extensions.telnet] INFO: Telnet Password: bbff65b506cc8ce7
2019-12-24 11:17:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:17:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:17:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:17:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:17:35 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:17:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:17:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:17:36 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:17:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:17:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:17:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6440,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.954738,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 17, 37, 904087),
 'log_count/DEBUG': 2,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 17, 35, 949349)}
2019-12-24 11:17:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 11:26:00 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:26:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:26:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:26:00 [scrapy.extensions.telnet] INFO: Telnet Password: c666f347624e3602
2019-12-24 11:26:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:26:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:26:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:26:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:26:01 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:26:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:26:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:26:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:26:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:26:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com> (referer: None)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 80, in parse
    raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))
NotImplementedError: BlogSpider.parse callback is not defined
2019-12-24 11:26:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:26:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6547,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.839054,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 26, 2, 876280),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NotImplementedError': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 26, 1, 37226)}
2019-12-24 11:26:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 11:26:17 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:26:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:26:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:26:17 [scrapy.extensions.telnet] INFO: Telnet Password: cd3d60dc3429e93b
2019-12-24 11:26:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:26:18 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:26:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:26:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:26:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:26:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com> (referer: None)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 80, in parse
    raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))
NotImplementedError: BlogSpider.parse callback is not defined
2019-12-24 11:26:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:26:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6444,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 2.067458,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 26, 20, 288986),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NotImplementedError': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 26, 18, 221528)}
2019-12-24 11:26:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 11:26:33 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:26:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:26:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:26:33 [scrapy.extensions.telnet] INFO: Telnet Password: 68f330071c5919e2
2019-12-24 11:26:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:26:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:26:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:26:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:26:33 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:26:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:26:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:26:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:26:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:26:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com> (referer: None)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 80, in parse
    raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))
NotImplementedError: BlogSpider.parse callback is not defined
2019-12-24 11:26:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:26:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6536,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.857307,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 26, 35, 753673),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NotImplementedError': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 26, 33, 896366)}
2019-12-24 11:26:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 11:26:51 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 11:26:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 11:26:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 11:26:51 [scrapy.extensions.telnet] INFO: Telnet Password: 64301dbe05a20d72
2019-12-24 11:26:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 11:26:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 11:26:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 11:26:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 11:26:51 [scrapy.core.engine] INFO: Spider opened
2019-12-24 11:26:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 11:26:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 11:26:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 11:26:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 11:26:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 11:26:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6508,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.383277,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 3, 26, 53, 243298),
 'log_count/DEBUG': 2,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 12, 24, 3, 26, 51, 860021)}
2019-12-24 11:26:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:00:48 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:00:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:00:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:00:48 [scrapy.extensions.telnet] INFO: Telnet Password: 5c9a4f110393d9d1
2019-12-24 13:00:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:00:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:00:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:00:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:00:49 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:00:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:00:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:00:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:00:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:00:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:00:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:00:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 17, in sub_article
    soup = BeautifulSoup(response.tetx,'html.parser')
AttributeError: 'HtmlResponse' object has no attribute 'tetx'
2019-12-24 13:01:03 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-12-24 13:01:03 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-12-24 13:01:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-12-24 13:01:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.teamssix.com/year/191201-220910.html> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2019-12-24 13:01:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 59629,
 'downloader/response_count': 11,
 'downloader/response_status_count/200': 10,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 15.505466,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 1, 4, 909949),
 'log_count/DEBUG': 12,
 'log_count/ERROR': 9,
 'log_count/INFO': 12,
 'request_depth_max': 1,
 'response_received_count': 11,
 'retry/count': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 12,
 'scheduler/enqueued/memory': 12,
 'spider_exceptions/AttributeError': 9,
 'start_time': datetime.datetime(2019, 12, 24, 5, 0, 49, 404483)}
2019-12-24 13:01:04 [scrapy.core.engine] INFO: Spider closed (shutdown)
2019-12-24 13:01:10 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:01:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:01:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:01:10 [scrapy.extensions.telnet] INFO: Telnet Password: 26daa808ff4d8dd6
2019-12-24 13:01:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:01:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:01:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:01:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:01:11 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:01:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:01:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:01:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:01:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:01:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:01:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64294,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 4.166839,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 1, 15, 980296),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 1, 11, 813457)}
2019-12-24 13:01:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:04:45 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:04:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:04:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:04:45 [scrapy.extensions.telnet] INFO: Telnet Password: 83baa1c6ade15a14
2019-12-24 13:04:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:04:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:04:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:04:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:04:46 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:04:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:04:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:04:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:04:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64396,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.154551,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 4, 49, 745146),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 4, 46, 590595)}
2019-12-24 13:04:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:05:37 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:05:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:05:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:05:37 [scrapy.extensions.telnet] INFO: Telnet Password: 0effb0d84840a7e8
2019-12-24 13:05:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:05:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:05:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:05:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:05:37 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:05:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:05:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:05:38 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:05:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:05:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:05:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64408,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 8.037493,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 5, 45, 942690),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 5, 37, 905197)}
2019-12-24 13:05:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:06:05 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:06:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:06:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:06:05 [scrapy.extensions.telnet] INFO: Telnet Password: aa7165aceb28f81a
2019-12-24 13:06:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:06:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:06:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:06:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:06:06 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:06:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:06:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:06:07 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:06:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:06:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:06:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64310,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 2.832422,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 6, 9, 642923),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 6, 6, 810501)}
2019-12-24 13:06:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:08:08 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:08:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:08:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:08:08 [scrapy.extensions.telnet] INFO: Telnet Password: e266bcdc101f7f08
2019-12-24 13:08:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:08:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:08:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:08:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:08:09 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:08:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:08:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:08:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:08:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 22, in sub_article
    exit()
  File "c:\programdata\anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
2019-12-24 13:08:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:08:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64468,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.469781,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 8, 12, 997949),
 'log_count/DEBUG': 12,
 'log_count/ERROR': 10,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'spider_exceptions/SystemExit': 10,
 'start_time': datetime.datetime(2019, 12, 24, 5, 8, 9, 528168)}
2019-12-24 13:08:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:08:29 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:08:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:08:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:08:29 [scrapy.extensions.telnet] INFO: Telnet Password: 317a058d9cb8b3c6
2019-12-24 13:08:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:08:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:08:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:08:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:08:30 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:08:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:08:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:08:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:08:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\spiders\teamssix_blog_spider.py", line 23, in sub_article
    sys.exit()
SystemExit
2019-12-24 13:08:33 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:08:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64422,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.353026,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 8, 33, 794242),
 'log_count/DEBUG': 12,
 'log_count/ERROR': 10,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'spider_exceptions/SystemExit': 10,
 'start_time': datetime.datetime(2019, 12, 24, 5, 8, 30, 441216)}
2019-12-24 13:08:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:09:52 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:09:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:09:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:09:52 [scrapy.extensions.telnet] INFO: Telnet Password: 973034ff299d9d0c
2019-12-24 13:09:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:09:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:09:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:09:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:09:53 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:09:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:09:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:09:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:09:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:07 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:10:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64371,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 13.758185,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 10, 7, 35772),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 9, 53, 277587)}
2019-12-24 13:10:07 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:10:42 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:10:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:10:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:10:42 [scrapy.extensions.telnet] INFO: Telnet Password: 9664563c01b99260
2019-12-24 13:10:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:10:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:10:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:10:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:10:44 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:10:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:10:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:10:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:10:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:10:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:10:50 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:10:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64386,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 5.862904,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 10, 50, 100046),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 10, 44, 237142)}
2019-12-24 13:10:50 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:11:20 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:11:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:11:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:11:20 [scrapy.extensions.telnet] INFO: Telnet Password: ba129d6d0604e5be
2019-12-24 13:11:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:11:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:11:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:11:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:11:20 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:11:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:11:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:11:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:11:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:11:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:11:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64371,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.282218,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 11, 24, 281592),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 11, 20, 999374)}
2019-12-24 13:11:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:12:05 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:12:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:12:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:12:06 [scrapy.extensions.telnet] INFO: Telnet Password: fb851d04e54bf8ba
2019-12-24 13:12:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:12:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:12:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:12:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:12:06 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:12:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:12:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:12:07 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:12:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:12:10 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:12:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64456,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.68215,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 12, 10, 487966),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 12, 6, 805816)}
2019-12-24 13:12:10 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:24:46 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:24:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:24:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:24:46 [scrapy.extensions.telnet] INFO: Telnet Password: 1d74f110bd71bdcc
2019-12-24 13:24:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:24:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:24:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:24:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:24:47 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:24:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:24:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:24:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:24:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:24:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64274,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 4.333409,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 24, 51, 860628),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 24, 47, 527219)}
2019-12-24 13:24:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:25:21 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:25:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:25:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:25:21 [scrapy.extensions.telnet] INFO: Telnet Password: 6f43ca6452dd6b8c
2019-12-24 13:25:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:25:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:25:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:25:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:25:22 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:25:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:25:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:25:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:25:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:25:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:25:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64385,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.214399,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 25, 25, 807803),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 25, 22, 593404)}
2019-12-24 13:25:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:25:43 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:25:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:25:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:25:43 [scrapy.extensions.telnet] INFO: Telnet Password: 362677cd2209953f
2019-12-24 13:25:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:25:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:25:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:25:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:25:44 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:25:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:25:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:25:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:25:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:25:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:25:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64388,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.379957,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 25, 47, 704220),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 25, 44, 324263)}
2019-12-24 13:25:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:26:51 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:26:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:26:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:26:51 [scrapy.extensions.telnet] INFO: Telnet Password: 61fd2472afba5027
2019-12-24 13:26:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:26:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:26:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:26:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:26:52 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:26:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:26:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:26:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:26:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:26:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:26:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:26:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64490,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.461739,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 26, 55, 723761),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 26, 52, 262022)}
2019-12-24 13:26:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:29:13 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:29:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:29:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 9d32103e4b1b4d85
2019-12-24 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:29:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:29:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:29:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:29:14 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:29:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:29:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:29:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:29:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:29:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:29:56 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-12-24 13:29:56 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-12-24 13:30:06 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:30:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:30:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:30:06 [scrapy.extensions.telnet] INFO: Telnet Password: ed306dc5509c7959
2019-12-24 13:30:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:30:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:30:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:30:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:30:07 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:30:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:30:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2019-12-24 13:30:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:30:11 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:30:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64294,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.292567,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 30, 11, 67105),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 30, 7, 774538)}
2019-12-24 13:30:11 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 13:30:14 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:30:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.teamssix.com/year/191127-201443.html> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2019-12-24 13:30:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.teamssix.com/year/191201-220910.html> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2019-12-24 13:30:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 55237,
 'downloader/response_count': 10,
 'downloader/response_status_count/200': 9,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 78.305113,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 30, 33, 262331),
 'log_count/DEBUG': 12,
 'log_count/INFO': 12,
 'request_depth_max': 1,
 'response_received_count': 10,
 'retry/count': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'start_time': datetime.datetime(2019, 12, 24, 5, 29, 14, 957218)}
2019-12-24 13:30:33 [scrapy.core.engine] INFO: Spider closed (shutdown)
2019-12-24 13:37:15 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 13:37:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 13:37:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 13:37:15 [scrapy.extensions.telnet] INFO: Telnet Password: a1e24e411ee88d77
2019-12-24 13:37:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 13:37:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 13:37:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 13:37:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 13:37:17 [scrapy.core.engine] INFO: Spider opened
2019-12-24 13:37:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 13:37:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 13:37:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 13:37:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 13:37:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64373,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.447649,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 5, 37, 20, 468561),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 5, 37, 17, 20912)}
2019-12-24 13:37:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:14:44 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:14:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:14:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:14:44 [scrapy.extensions.telnet] INFO: Telnet Password: 81267a3ca11244fb
2019-12-24 14:14:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:14:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:14:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:14:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:14:45 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:14:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:14:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:14:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:14:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:14:48 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:14:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64448,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 2.845856,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 14, 48, 362401),
 'log_count/DEBUG': 12,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 14, 45, 516545)}
2019-12-24 14:14:48 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:42:28 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:42:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:42:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_FORMAT': 'json', 'FEED_URI': 'result.json', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:42:28 [scrapy.extensions.telnet] INFO: Telnet Password: 328f712f6fed2876
2019-12-24 14:42:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:42:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:42:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:42:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:42:29 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:42:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:42:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:42:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:42:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:42:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:42:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:42:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:42:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:42:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:42:32 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: result.json
2019-12-24 14:42:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64543,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.276064,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 42, 32, 819494),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 42, 29, 543430)}
2019-12-24 14:42:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:45:54 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:45:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:45:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_FORMAT': 'csv', 'FEED_URI': 'result.csv', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:45:54 [scrapy.extensions.telnet] INFO: Telnet Password: b0bf5a46579f4270
2019-12-24 14:45:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:45:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:45:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:45:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:45:54 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:45:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:45:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:45:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:45:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:45:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:45:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:46:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:46:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:46:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:46:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:46:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:46:02 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: result.csv
2019-12-24 14:46:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64491,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 7.809163,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 46, 2, 593704),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 45, 54, 784541)}
2019-12-24 14:46:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:47:01 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:47:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:47:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_FORMAT': 'xml', 'FEED_URI': 'result.xml', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:47:01 [scrapy.extensions.telnet] INFO: Telnet Password: 1163560b59bc2528
2019-12-24 14:47:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:47:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:47:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:47:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:47:02 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:47:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:47:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:47:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:47:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:47:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:47:05 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:47:05 [scrapy.extensions.feedexport] INFO: Stored xml feed (10 items) in: result.xml
2019-12-24 14:47:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64616,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.549265,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 47, 5, 936745),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 47, 2, 387480)}
2019-12-24 14:47:05 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:50:48 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:50:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:50:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'FEED_FORMAT': 'csv', 'FEED_URI': 'result2.csv', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:50:48 [scrapy.extensions.telnet] INFO: Telnet Password: d2fd28a76af5ecd2
2019-12-24 14:50:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:50:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:50:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:50:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:50:48 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:50:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:50:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:50:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:50:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:50:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:50:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:50:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:50:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:50:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:50:52 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: result2.csv
2019-12-24 14:50:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64488,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.428804,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 50, 52, 72942),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 50, 48, 644138)}
2019-12-24 14:50:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:55:57 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:55:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:55:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'FEED_FORMAT': 'json', 'FEED_URI': 'result2.json', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:55:57 [scrapy.extensions.telnet] INFO: Telnet Password: bc9eb2de2f663ddd
2019-12-24 14:55:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:55:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:55:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:55:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:55:58 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:55:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:55:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:55:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:56:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:56:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:56:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:56:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:56:02 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: result2.json
2019-12-24 14:56:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64415,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 3.585409,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 56, 2, 315749),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 55, 58, 730340)}
2019-12-24 14:56:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:56:57 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:56:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:56:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'FEED_URI': 'result2.jsonlines', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:56:57 [scrapy.extensions.telnet] INFO: Telnet Password: d6261bc81affb03d
2019-12-24 14:56:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:56:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:56:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:56:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:56:58 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:56:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:56:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:56:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:56:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:57:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:57:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:57:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:57:02 [scrapy.extensions.feedexport] INFO: Stored jsonlines feed (10 items) in: result2.jsonlines
2019-12-24 14:57:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64424,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 4.37983,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 57, 2, 924782),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 56, 58, 544952)}
2019-12-24 14:57:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:57:50 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:57:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:57:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'FEED_FORMAT': 'jl', 'FEED_URI': 'result2.jl', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:57:51 [scrapy.extensions.telnet] INFO: Telnet Password: 5cc53e2da35fb32b
2019-12-24 14:57:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:57:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:57:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:57:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:57:51 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:57:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:57:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:57:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:57:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:57:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:57:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:57:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:58:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:58:00 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:58:00 [scrapy.extensions.feedexport] INFO: Stored jl feed (10 items) in: result2.jl
2019-12-24 14:58:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64249,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 8.929431,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 58, 0, 498783),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 57, 51, 569352)}
2019-12-24 14:58:00 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 14:58:29 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 14:58:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 14:58:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'FEED_FORMAT': 'csv', 'FEED_URI': 'result2.csv', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 14:58:29 [scrapy.extensions.telnet] INFO: Telnet Password: 0fdeedacfdf320c0
2019-12-24 14:58:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 14:58:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 14:58:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 14:58:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-12-24 14:58:30 [scrapy.core.engine] INFO: Spider opened
2019-12-24 14:58:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 14:58:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 14:58:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 14:58:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 14:58:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-093319.html>
{'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201438.html>
{'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201443.html>
{'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191201-220910.html>
{'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191127-201447.html>
{'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161533.html>
{'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191220-161745.html>
{'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191206-172901.html>
{'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191222-192227.html>
{'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
2019-12-24 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.teamssix.com/year/191224-092208.html>
{'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
2019-12-24 14:58:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 14:58:32 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: result2.csv
2019-12-24 14:58:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3852,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 64560,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 2.657383,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 6, 58, 32, 726235),
 'item_scraped_count': 10,
 'log_count/DEBUG': 22,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 12,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 12, 24, 6, 58, 30, 68852)}
2019-12-24 14:58:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-12-24 16:20:25 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)
2019-12-24 16:20:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0
2019-12-24 16:20:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'teamssix', 'FEED_EXPORT_ENCODING': 'gb18030', 'LOG_FILE': 'all.log', 'NEWSPIDER_MODULE': 'teamssix.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['teamssix.spiders']}
2019-12-24 16:20:25 [scrapy.extensions.telnet] INFO: Telnet Password: 19ad89bb4f3e390b
2019-12-24 16:20:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-12-24 16:20:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-12-24 16:20:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-12-24 16:20:26 [scrapy.middleware] INFO: Enabled item pipelines:
['teamssix.pipelines.TeamssixPipeline']
2019-12-24 16:20:26 [scrapy.core.engine] INFO: Spider opened
2019-12-24 16:20:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-12-24 16:20:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-12-24 16:20:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.teamssix.com/robots.txt> (referer: None)
2019-12-24 16:20:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com> (referer: None)
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-093319.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191224-093319.html',
 'list': ['0x00 新建项目', '0x01 创建一个爬虫', '0x02 运行爬虫', '0x03 爬取内容解析'],
 'title': '【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161745.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191201-220910.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191206-172901.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191220-161745.html',
 'list': ['0x00 前言', '0x01 基本用法', '0x02 aiohttp的使用'],
 'title': '【Python 学习笔记】 异步IO (asyncio) 协程'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191201-220910.html',
 'list': [],
 'title': '【直播笔记】白帽子的成长之路'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191206-172901.html',
 'list': ['0x00 漏洞描述',
          '0x01 漏洞利用',
          '1、Windows下使用nslookup',
          '2、Kali下使用dig、dnsenum、dnswalk',
          'a、dig',
          'b、dnsenum',
          'c、dnswalk',
          '0x02 修复建议',
          '0x03 总结'],
 'title': '【漏洞复现】DNS域传送漏洞'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191220-161533.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191222-192227.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191220-161533.html',
 'list': ['0x00 前言',
          '0x01 什么是多进程爬虫',
          '0x02 准备工作',
          '0x03 测试普通爬取方法',
          '0x04 测试多进程爬取方法',
          '0x05 处理密集计算任务耗时对比'],
 'title': '【Python 学习笔记】多进程爬虫'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191222-192227.html',
 'list': ['0x00 漏洞说明',
          '0x01 漏洞影响',
          '0x02 漏洞发现',
          '0x03 漏洞验证',
          '0x04 漏洞利用',
          '1、一个简单的测试靶场',
          '2、Weblogic漏洞复现',
          '1、漏洞存在测试',
          '2、通过Redis服务反弹shell',
          '0x05 绕过技巧',
          '0x06 SSRF防御'],
 'title': '【漏洞笔记】浅谈SSRF原理及其利用'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191224-092208.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:31 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191224-092208.html',
 'list': ['0x00 简介',
          'Engine',
          'Scheduler',
          'Downloader',
          'Spiders',
          'Item Pipelines',
          '0x01 安装'],
 'title': '【Python Scrapy 爬虫框架】 1、简介与安装'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201447.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201443.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:31 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191127-201447.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Robots.txt站点文件'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:31 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191127-201443.html',
 'list': ['0x00 概述', '0x01 漏洞描述', '0x02 漏洞危害', '0x03 修复建议'],
 'title': '【漏洞笔记】Host头攻击'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.teamssix.com/year/191127-201438.html> (failed 1 times): TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2019-12-24 16:20:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.teamssix.com/year/191127-201438.html> (referer: https://www.teamssix.com)
2019-12-24 16:20:51 [scrapy.core.scraper] ERROR: Error processing {'id': 'https://www.teamssix.com/year/191127-201438.html',
 'list': ['0x00 概述',
          '0x01 GET',
          '0x02 HEAD',
          '0x03 POST',
          '0x04 PUT',
          '0x05 DELETE',
          '0x06 CONNECT',
          '0x07 OPTIONS',
          '0x08 TRACE',
          '0x09 PATCH'],
 'title': '【经验总结】常见的HTTP方法'}
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Dora\Desktop\scrapy\teamssix\teamssix\pipelines.py", line 25, in process_item
    self.collection.insert_one(dict(item))
AttributeError: 'TeamssixPipeline' object has no attribute 'collection'
2019-12-24 16:20:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-12-24 16:20:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 4187,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 64456,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 11,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 24.674105,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 12, 24, 8, 20, 51, 208875),
 'log_count/DEBUG': 13,
 'log_count/ERROR': 10,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 12,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 1,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 12,
 'scheduler/dequeued/memory': 12,
 'scheduler/enqueued': 12,
 'scheduler/enqueued/memory': 12,
 'start_time': datetime.datetime(2019, 12, 24, 8, 20, 26, 534770)}
2019-12-24 16:20:51 [scrapy.core.engine] INFO: Spider closed (finished)
